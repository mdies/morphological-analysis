{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnlisHjUUtlM"
   },
   "source": [
    "# Snake identification challenge \n",
    "\n",
    "Using transfer learning as in Activity 7 to start with. See full explanation for snake identification challenge [here](https://www.aicrowd.com/challenges/snake-species-identification-challenge#task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1LmKoW84dra"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/io/_video_opt.py:17: UserWarning: video reader based on ffmpeg c++ ops not available\n",
      "  warnings.warn(\"video reader based on ffmpeg c++ ops not available\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"You should enagle GPU in the Runtime menu\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import skimage.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "dataset_dir = 'Images/' # '/datalab/train/'\n",
    "\n",
    "train_dir = 'Images/train_images/'\n",
    "\n",
    "df_train_labels = pd.read_csv(dataset_dir + 'train_labels.csv')\n",
    "\n",
    "dictionary = pd.factorize(df_train_labels.scientific_name)\n",
    "\n",
    "df_train_labels['labels'] = dictionary[0]\n",
    "\n",
    "dict_labels = df_train_labels[['scientific_name', 'labels']].drop_duplicates()\n",
    "\n",
    "# str(train_dir) + str(df_train_labels.hashed_id[0]) + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images_small\n",
    "# test_metadata_small.csv\n",
    "challenge_dir = 'Images/test_images_small/'\n",
    "\n",
    "df_challenge_labels = pd.read_csv(dataset_dir + 'test_metadata_small.csv') # NO LABELS IN CHALLENGE DATA SET!\n",
    "\n",
    "#pd.merge(df_challenge_labels, dict_labels, left_on=['scientific_name'], right_on=['scientific_name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating corrupted images from image list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(dataset_dir + 'train_labels_proc.csv'):\n",
    "    # True\n",
    "    # Loac csv file containing names and labels for non-corrupted images\n",
    "    df = pd.read_csv(dataset_dir + 'train_labels_proc.csv')\n",
    "else:\n",
    "    # False\n",
    "    # Create csv file containing names and labels for non-corrupted images\n",
    "    df = df_train_labels.copy()\n",
    "\n",
    "    for item in df.hashed_id:\n",
    "        try:\n",
    "            # myimage= Image.open(str(train_dir_ori) + str(df_train_labels.hashed_id[0]) + \".jpg\")\n",
    "            myimage= Image.open(str(train_dir) + str(item) + \".jpg\") # Alternativa Eva: mirar el tamany de la imatge i x\n",
    "        except IOError:                                              # filtrar aquelles que siguin < XX MB.\n",
    "            print (\"cannot identify image file\", myimage)\n",
    "            df = df[df.hashed_id != item]\n",
    "\n",
    "\n",
    "    df.to_csv(dataset_dir + 'train_labels_proc.csv') # KEEP THIS ONE AND LOAD IT (do not execute preprocessing twice!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(challenge_dir + 'test_metadata_small.csv'):\n",
    "    # True\n",
    "    # Loac csv file containing names and labels for non-corrupted images\n",
    "    df_challenge = pd.read_csv(challenge_dir + 'test_metadata_small.csv')\n",
    "else:\n",
    "    # False\n",
    "    # Create csv file containing names and labels for non-corrupted images\n",
    "    df_challenge = df_challenge_labels.copy()\n",
    "\n",
    "    for item in df_challenge.hashed_id:\n",
    "        try:\n",
    "            # myimage= Image.open(str(train_dir_ori) + str(df_train_labels.hashed_id[0]) + \".jpg\")\n",
    "            myimage= Image.open(str(challenge_dir) + str(item) + \".jpg\") # Alternativa Eva: mirar el tamany de la imatge i x\n",
    "        except IOError:                                              # filtrar aquelles que siguin < XX MB.\n",
    "            print (\"cannot identify image file\", myimage)\n",
    "            df_challenge = df_challenge[df_challenge.hashed_id != item]\n",
    "\n",
    "\n",
    "    df_challenge.to_csv(dataset_dir + 'challenge_labels_proc.csv') # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train, test, and validation data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a good DataLoader example [here!!!](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.hashed_id[0:1000] # For code testing purposes only!\n",
    "# y = df.labels[0:1000]\n",
    "X = df.hashed_id\n",
    "y = df.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # test_size=0.2\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) # test_size=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train + val + test (random 70-20-10)\n",
    "# df['hashed_id'].sample(n=3, random_state = 1)\n",
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "# train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103822"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25696"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeDataset():\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X_partN, y_partN, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_partN (array): Array with all image names for a given data set (train, test, or val).\n",
    "            y_partN (array): Array with the corresponding labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.DataFrame({'x':X_partN, 'y':y_partN}) # X_train, y_train\n",
    "        self.root_dir = root_dir # path imatge al disc 'train_dir'\n",
    "        self.transform = transform # transformacions a aplicar\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data.iloc[idx, 0]+'.jpg')\n",
    "        # BUG found for skimage: https://github.com/numpy/numpy/issues/12744\n",
    "        # image = io.imread(img_name)\n",
    "        image = Image.open(img_name) # Image.open(FILENAME).convert('L')\n",
    "        image = image.convert('RGB')\n",
    "        labels = self.data.iloc[idx, 1]\n",
    "        sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            # Transform to tensors with pytorch\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            # From https://discuss.pytorch.org/t/change-labels-in-data-loader/36823/5\n",
    "            # sample['labels'] = torch.tensor(sample['labels']) \n",
    "            # From https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type-float-when-using-crossentropyloss/30542/2\n",
    "            # sample['labels'] = torch.tensor(sample['labels'], dtype=torch.long, device=device) \n",
    "            # From https://discuss.pytorch.org/t/expected-object-of-scalar-type-long-but-got-scalar-type-float-for-argument-2-target/33102\n",
    "            # sample['labels'] = torch.tensor(sample['labels'], dtype=torch.int64, device=device) \n",
    "            sample['labels'] = torch.tensor(sample['labels'], dtype=torch.float, device=device) \n",
    "            \n",
    "                \n",
    "        return sample\n",
    "    \n",
    "    # IF DATASET = EVAL => RETURN SAMPLE, IMAGE_PATH (EVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO!!!!** Check whether challenge images are part of training data set AND REMOVE THEM IF SO!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to transform input images to parameters used to trained VGG16!!!\n",
    "# https://medium.com/datadriveninvestor/creating-a-pytorch-image-classifier-da9db139ba80\n",
    "transform = transforms.Compose([# size must be >= 224 \n",
    "                                transforms.Resize(256), \n",
    "                                transforms.CenterCrop(224), \n",
    "                                transforms.ToTensor(), # Convert the image to a tensor with pixels in the range [0, 1]\n",
    "                                # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with Imagenet parameters\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SnakeDataset(X_train, y_train, root_dir = train_dir, transform=transform)\n",
    "testset = SnakeDataset(X_test, y_test, root_dir = train_dir, transform=transform)\n",
    "valset = SnakeDataset(X_val, y_val, root_dir = train_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the sizes of first 4 samples and showing the images\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(trainset)):\n",
    "    sample = trainset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['labels'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(Image.open(os.path.join(train_dir, X_train.iloc[i]+'.jpg')))\n",
    "    print(\"Labelled as: \" + str(y_train.iloc[i]))\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some hyper-parameters\n",
    "hparams = {\n",
    "    'batch_size': 100, #  40, \n",
    "    'num_epochs':12,\n",
    "    'val_batch_size': 100, #  40, \n",
    "    'learning_rate':1e-3,\n",
    "    'log_interval':100,\n",
    "}\n",
    "\n",
    "# we select to work on GPU if it is available in the machine, otherwise\n",
    "# will run on CPU\n",
    "hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=hparams['batch_size'], \n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=hparams['val_batch_size'], \n",
    "    shuffle=False)\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    batch_size=hparams['val_batch_size'], \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we can sample a BATCH from the dataloader by running over its iterator\n",
    "bimg, blabel = next(iter(train_loader))['image'], next(iter(train_loader))['labels']\n",
    "print('Batch Img shape: ', bimg.shape)\n",
    "print('Batch Label shape: ', blabel.shape)\n",
    "print('The Batched tensors return a collection of {} RGB images ({} channel, {} height pixels, {} width pixels)'.format(bimg.shape[0],\n",
    "                                                                                                                        bimg.shape[1],\n",
    "                                                                                                                        bimg.shape[2],\n",
    "                                                                                                                        bimg.shape[3]))\n",
    "print('In the case of the labels, we obtain {} batched integers, one per image'.format(blabel.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-trained VGG16 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "pretrained_model = vgg16(pretrained=True)\n",
    "pretrained_model.eval()\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = pretrained_model.features\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by simply running instances of the previously-introduced ImageFolder Dataset to extract features from these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataset, loader, batch_size): # dataset = trainset, testset, valset; loader = train_loader, test_loader, val_loader\n",
    "                                                   # batch_size = hparams['batch_size']\n",
    "    features = np.zeros(shape=(len(dataset), 512, 7, 7)) # 4, 4\n",
    "    labels = np.zeros(shape=(len(dataset),))\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(loader):\n",
    "            # print(i)\n",
    "            inputs_batch = sample_batched['image']\n",
    "            labels_batch = sample_batched['labels']\n",
    "            inputs_batch, labels_batch = inputs_batch.to(device), labels_batch.to(device)\n",
    "            # print(\"Entering feature_extractor function\")\n",
    "            features_batch = feature_extractor(inputs_batch)\n",
    "            # print(i * batch_size)\n",
    "            # print((i + 1) * batch_size)\n",
    "            # print(features_batch.cpu().detach().numpy().shape)\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            #features[i * batch_size : (i + 1) * batch_size] = features_batch.cpu().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            #labels[i * batch_size : (i + 1) * batch_size] = labels_batch.cpu().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(trainset, train_loader, hparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_features, test_labels = extract_features(testset, test_loader, hparams['val_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features, validation_labels = extract_features(valset, eval_loader, hparams['val_batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted features are currently of shape (samples, 512, 4, 4). We will feed them to a densely-connected classifier, so first we must flatten them to (samples, 8192):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (-1, 7 * 7 * 512)) # 4 * 4\n",
    "test_features = np.reshape(test_features, (-1, 7 * 7 * 512)) # 4 * 4\n",
    "validation_features = np.reshape(validation_features, (-1, 7 * 7 * 512)) # 4 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the classifier top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a total of 'len(dict_labels)' different labels to classify your images into. \n",
    "# MULTIPLE CLASSIFICATOR!\n",
    "\n",
    "feature_classifier = nn.Sequential(\n",
    "        nn.Linear(7*7*512, 256), # nn.Linear(8 * 8 * 64, 512), # 4*4*\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256,len(dict_labels)), # Number of different labels to classify our images into\n",
    "        nn.LogSoftmax(dim=-1) # From Activitat 6\n",
    ")\n",
    "\n",
    "feature_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(feature_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# As we are dealing here with a multiple class problem, we need to use torch.nn.CrossEntropyLoss() as the loss\n",
    "# function (see https://stackoverflow.com/questions/56821729/pytorch-bceloss-valueerror-target-and-input-must-have-the-same-number-of-ele).\n",
    "# loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, loss_fn, train_loader, val_loader, epochs):\n",
    "\n",
    "    train_accuracies, train_losses, val_accuracies, val_losses = [], [], [], []\n",
    "    val_loss = AverageMeter()\n",
    "    val_accuracy = AverageMeter()\n",
    "    train_loss = AverageMeter()\n",
    "    train_accuracy = AverageMeter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss.reset()\n",
    "        train_accuracy.reset()\n",
    "        train_loop = tqdm(train_loader, unit=\" batches\")  # For printing the progress bar\n",
    "        for data, target in train_loop:\n",
    "            train_loop.set_description('[TRAIN] Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "            data, target = data.float().to(device), target.float().to(device)\n",
    "            target = target.unsqueeze(-1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            target = target.squeeze(1)\n",
    "            \n",
    "            loss = loss_fn(output.double(), target.long())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.update(loss.item(), n=len(target))\n",
    "            \n",
    "            pred = output.round()  # get the prediction\n",
    "            \n",
    "            class_pred=torch.argmax(pred, dim=1)\n",
    "            \n",
    "            acc = (class_pred==target).sum().item()/len(target)\n",
    "            \n",
    "            train_accuracy.update(acc, n=len(target))\n",
    "            train_loop.set_postfix(loss=train_loss.avg, accuracy=train_accuracy.avg)\n",
    "\n",
    "        train_losses.append(train_loss.avg)\n",
    "        train_accuracies.append(train_accuracy.avg)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss.reset()\n",
    "        val_accuracy.reset()\n",
    "        val_loop = tqdm(val_loader, unit=\" batches\")  # For printing the progress bar\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loop:\n",
    "                val_loop.set_description('[VAL] Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "                data, target = data.float().to(device), target.float().to(device)\n",
    "                target = target.unsqueeze(-1)\n",
    "                #target = target.squeeze(1)\n",
    "                output = model(data)\n",
    "                target = target.squeeze(1)\n",
    "                \n",
    "                loss = loss_fn(output.double(), target.long())\n",
    "                val_loss.update(loss.item(), n=len(target))\n",
    "                \n",
    "                pred = output.round()  # get the prediction\n",
    "                #acc = pred.eq(target.view_as(pred)).sum().item()/len(target)\n",
    "                class_pred=torch.argmax(pred, dim=1)\n",
    "                acc = (class_pred==target).sum().item()/len(target)\n",
    "                #acc = pred.eq(target.view_as(pred)).sum().item()/len(target)\n",
    "                val_accuracy.update(acc, n=len(target))\n",
    "                val_loop.set_postfix(loss=val_loss.avg, accuracy=val_accuracy.avg)\n",
    "\n",
    "        val_losses.append(val_loss.avg)\n",
    "        val_accuracies.append(val_accuracy.avg)\n",
    "        \n",
    "    return train_accuracies, train_losses, val_accuracies, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_features_dataset = TensorDataset(torch.tensor(train_features), torch.tensor(train_labels, dtype=torch.float, device=device)) # dtype=torch.long\n",
    "train_features_loader = DataLoader(train_features_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "\n",
    "test_features_dataset = TensorDataset(torch.tensor(test_features), torch.tensor(test_labels, dtype=torch.float64, device=device))\n",
    "test_features_loader = DataLoader(test_features_dataset, batch_size=hparams['val_batch_size'], shuffle=False)\n",
    "\n",
    "val_features_dataset = TensorDataset(torch.tensor(validation_features), torch.tensor(validation_labels, dtype=torch.float64, device=device))\n",
    "val_features_loader = DataLoader(val_features_dataset, batch_size=hparams['val_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_accuracies, train_losses, val_accuracies, val_losses = train_model(feature_classifier, optimizer, loss_fn, \n",
    "                                                                         train_features_loader, val_features_loader, \n",
    "                                                                         hparams['num_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(train_accuracies))\n",
    "\n",
    "plt.plot(epochs, train_accuracies, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check [this](https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864)!!! It's training beautifully explained!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeDataset_challenge():\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X_partN, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_partN (array): Array with all image names for a given data set (train, test, or val).\n",
    "            y_partN (array): Array with the corresponding labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.DataFrame(X_partN) # X_train, y_train\n",
    "        self.root_dir = root_dir # path imatge al disc 'train_dir'\n",
    "        self.transform = transform # transformacions a aplicar\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data.iloc[idx, 0]+'.jpg')\n",
    "        # BUG found for skimage: https://github.com/numpy/numpy/issues/12744\n",
    "        # image = io.imread(img_name)\n",
    "        image = Image.open(img_name) # Image.open(FILENAME).convert('L')\n",
    "        image = image.convert('RGB')\n",
    "        #labels = self.data.iloc[idx, 1]\n",
    "        # sample = image\n",
    "        sample = {'image': image, 'hashed_id': self.data.iloc[idx, 0]}\n",
    "\n",
    "        if self.transform:\n",
    "            # Transform to tensors with pytorch\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            # sample['labels'] = torch.tensor(sample['labels'], dtype=torch.float, device=device) \n",
    "            \n",
    "        return sample #, self.data.iloc[idx, 0]\n",
    "    \n",
    "    # IF DATASET = EVAL => RETURN SAMPLE, IMAGE_PATH (EVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "# torch.save(feature_classifier.state_dict(), \"first_model.pt\")\n",
    "# np.savez('history_first_model.npz', train_accuracies=train_accuracies, train_losses=train_losses, val_accuracies=val_accuracies, val_losses=val_losses)\n",
    "torch.save(feature_classifier.state_dict(), \"first_model_v2.pt\")\n",
    "np.savez('history_first_model_v2.npz', train_accuracies=train_accuracies, train_losses=train_losses, val_accuracies=val_accuracies, val_losses=val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = feature_classifier #classificador\n",
    "model.load_state_dict(torch.load(\"first_model_v2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform challenge data set\n",
    "# SnakeDataset_challenge\n",
    "X_challenge = df_challenge.hashed_id\n",
    "challengeset = SnakeDataset_challenge(X_challenge, root_dir = challenge_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img, hashedid = challengeset[0]['image'], challengeset[0]['hashed_id']\n",
    "print('Img shape: ', img.shape)\n",
    "print('hashedid: ', hashedid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_loader = torch.utils.data.DataLoader(\n",
    "    challengeset,\n",
    "    batch_size=200, \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def challenge_extract_features(dataset, loader, batch_size): # dataset = trainset, testset, valset; loader = train_loader, test_loader, val_loader\n",
    "                                                   # batch_size = hparams['batch_size']\n",
    "    features = np.zeros(shape=(len(dataset), 512, 7, 7)) # 4, 4\n",
    "    # labels = np.zeros(shape=(len(dataset),))\n",
    "    hashedids = [] #np.zeros(shape=(len(dataset),))\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(loader):\n",
    "            # print(i)\n",
    "            inputs_batch = sample_batched['image']\n",
    "            #labels_batch = sample_batched['labels']\n",
    "            hashedids_batch = sample_batched['hashed_id']\n",
    "            # inputs_batch, labels_batch = inputs_batch.to(device), labels_batch.to(device)\n",
    "            inputs_batch = inputs_batch.to(device)\n",
    "            # print(\"Entering feature_extractor function\")\n",
    "            features_batch = feature_extractor(inputs_batch)\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            # labels[i * batch_size : (i + 1) * batch_size] = labels_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            # hashedids[i * batch_size : (i + 1) * batch_size] = hashedids_batch#.cpu().detach().numpy() #hashedids_batch.cpu().detach().numpy()\n",
    "            hashedids.extend(hashedids_batch)\n",
    "            \n",
    "    return features, hashedids #, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_features, challenge_hashedids = challenge_extract_features(challengeset, challenge_loader, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed classifier with challenge_features\n",
    "challenge_features = np.reshape(challenge_features, (-1, 7 * 7 * 512)) # 4 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "challenge_output = feature_classifier(torch.tensor(challenge_features, dtype=torch.float, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using `nn.LogSoftmax()` in the final layer of the classifier, and given that `LogSoftmax(x_i) = log(Softmax(x_i))`, and `Softmax(dim=-1)` gives the probabilities of belonging to considered classes for a given input (`x_i`) normalized to 1. If we want to recover the probabilities normalized to 1, we just need to do: `probability(x_i) = exp(LogSoftmax(x_i))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "challenge_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From https://www.programcreek.com/python/example/101149/torch.exp\n",
    "torch.exp(challenge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities normalized to 1\n",
    "challenge_probabilities = torch.exp(challenge_output)\n",
    "challenge_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most probable class with torch.argmax\n",
    "challenge_pred = challenge_output.round()  # get the prediction\n",
    "challenge_class_pred=torch.argmax(challenge_pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving output to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge_class_pred -> to name of snake (use dictionary)\n",
    "# challenge_probabilities\n",
    "# hashed_id\n",
    "#\n",
    "# Input: challenge_features, challenge_hashedids\n",
    "df_challenge['label_pred'] = challenge_class_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge = pd.merge(df_challenge, dict_labels, left_on=['label_pred'], right_on=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge = df_challenge.drop(columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge = df_challenge.rename(columns={\"scientific_name\": \"scientific_name_pred\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_cols = list(range(0, 85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probabilities = pd.DataFrame(challenge_probabilities.cpu().detach().numpy(), columns=prob_cols) # , columns=['a', 'b', 'c'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge_output = pd.concat([df_challenge, df_probabilities], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge_output.to_csv(dataset_dir + 'challenge_output_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge = pd.merge(df_challenge, dict_labels, left_on=['label_pred'], right_on=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob_cols = pd.DataFrame(prob_cols, columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob_cols = pd.merge(df_prob_cols, dict_labels, left_on=['labels'], right_on=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probabilities_names = pd.DataFrame(challenge_probabilities.cpu().detach().numpy(), columns=df_prob_cols['scientific_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge_output_names = pd.concat([df_challenge, df_probabilities_names], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge_output_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge_output_names.to_csv(dataset_dir + 'challenge_output_names_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the last 3 convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in feature_extractor[:24]:  # Freeze layers 0 to 23\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for layer in feature_extractor[24:]:  # Train layers 24 to 30\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        feature_extractor,\n",
    "        nn.Flatten(),\n",
    "        feature_classifier\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ft(model, optimizer, loss_fn, train_loader, val_loader, epochs):\n",
    "\n",
    "    train_accuracies, train_losses, val_accuracies, val_losses = [], [], [], []\n",
    "    val_loss = AverageMeter()\n",
    "    val_accuracy = AverageMeter()\n",
    "    train_loss = AverageMeter()\n",
    "    train_accuracy = AverageMeter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss.reset()\n",
    "        train_accuracy.reset()\n",
    "        train_loop = tqdm(train_loader, unit=\" batches\")  # For printing the progress bar\n",
    "        # for data, target in train_loop:\n",
    "        for item in train_loop:\n",
    "            train_loop.set_description('[TRAIN] Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "            data, target = item['image'].float().to(device), item['labels'].float().to(device)\n",
    "            target = target.unsqueeze(-1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            target = target.squeeze(1)\n",
    "            \n",
    "            loss = loss_fn(output.double(), target.long())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.update(loss.item(), n=len(target))\n",
    "            \n",
    "            pred = output.round()  # get the prediction\n",
    "            \n",
    "            class_pred=torch.argmax(pred, dim=1)\n",
    "            \n",
    "            acc = (class_pred==target).sum().item()/len(target)\n",
    "            \n",
    "            train_accuracy.update(acc, n=len(target))\n",
    "            train_loop.set_postfix(loss=train_loss.avg, accuracy=train_accuracy.avg)\n",
    "\n",
    "        train_losses.append(train_loss.avg)\n",
    "        train_accuracies.append(train_accuracy.avg)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss.reset()\n",
    "        val_accuracy.reset()\n",
    "        val_loop = tqdm(val_loader, unit=\" batches\")  # For printing the progress bar\n",
    "        with torch.no_grad():\n",
    "            for item in val_loop:\n",
    "                val_loop.set_description('[VAL] Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "                data, target = item['image'].float().to(device), item['labels'].float().to(device)\n",
    "                target = target.unsqueeze(-1)\n",
    "                output = model(data)\n",
    "                target = target.squeeze(1)\n",
    "                \n",
    "                loss = loss_fn(output.double(), target.long())\n",
    "                val_loss.update(loss.item(), n=len(target))\n",
    "                \n",
    "                pred = output.round()  # get the prediction\n",
    "                #acc = pred.eq(target.view_as(pred)).sum().item()/len(target)\n",
    "                class_pred=torch.argmax(pred, dim=1)\n",
    "                acc = (class_pred==target).sum().item()/len(target)\n",
    "                #acc = pred.eq(target.view_as(pred)).sum().item()/len(target)\n",
    "                val_accuracy.update(acc, n=len(target))\n",
    "                val_loop.set_postfix(loss=val_loss.avg, accuracy=val_accuracy.avg)\n",
    "\n",
    "        val_losses.append(val_loss.avg)\n",
    "        val_accuracies.append(val_accuracy.avg)\n",
    "        \n",
    "    return train_accuracies, train_losses, val_accuracies, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies_ft, train_losses_ft, test_accuracies_ft, test_losses_ft = train_model_ft(model, optimizer, loss_fn,\n",
    "                                                                                          train_loader, test_loader,\n",
    "                                                                                          hparams['num_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"second_model.pt\")\n",
    "# np.savez('history_second_model.npz', train_accuracies=train_accuracies_ft, train_losses=train_losses_ft, \n",
    "#          val_accuracies=test_accuracies_ft, val_losses=test_losses_ft)\n",
    "torch.save(model.state_dict(), \"second_model_v2.pt\")\n",
    "np.savez('history_second_model_v2.npz', train_accuracies=train_accuracies_ft, train_losses=train_losses_ft, \n",
    "         val_accuracies=test_accuracies_ft, val_losses=test_losses_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(train_accuracies_ft))\n",
    "\n",
    "plt.plot(epochs, train_accuracies_ft, 'b', label='Training acc')\n",
    "plt.plot(epochs, test_accuracies_ft, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_losses_ft, 'b', label='Training loss')\n",
    "plt.plot(epochs, test_losses_ft, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold out metrics - PENDING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeDataset():\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X_partN, y_partN, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_partN (array): Array with all image names for a given data set (train, test, or val).\n",
    "            y_partN (array): Array with the corresponding labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.DataFrame({'x':X_partN, 'y':y_partN}) # X_train, y_train\n",
    "        self.root_dir = root_dir # path imatge al disc 'train_dir'\n",
    "        self.transform = transform # transformacions a aplicar\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data.iloc[idx, 0]+'.jpg')\n",
    "        # BUG found for skimage: https://github.com/numpy/numpy/issues/12744\n",
    "        # image = io.imread(img_name)\n",
    "        image = Image.open(img_name) # Image.open(FILENAME).convert('L')\n",
    "        image = image.convert('RGB')\n",
    "        labels = self.data.iloc[idx, 1]\n",
    "        sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            # Transform to tensors with pytorch\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            # From https://discuss.pytorch.org/t/change-labels-in-data-loader/36823/5\n",
    "            # sample['labels'] = torch.tensor(sample['labels']) \n",
    "            # From https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type-float-when-using-crossentropyloss/30542/2\n",
    "            # sample['labels'] = torch.tensor(sample['labels'], dtype=torch.long, device=device) \n",
    "            # From https://discuss.pytorch.org/t/expected-object-of-scalar-type-long-but-got-scalar-type-float-for-argument-2-target/33102\n",
    "            # sample['labels'] = torch.tensor(sample['labels'], dtype=torch.int64, device=device) \n",
    "            sample['labels'] = torch.tensor(sample['labels'], dtype=torch.float, device=device) \n",
    "            \n",
    "                \n",
    "        return sample\n",
    "    \n",
    "    # IF DATASET = EVAL => RETURN SAMPLE, IMAGE_PATH (EVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to transform input images to parameters used to trained VGG16!!!\n",
    "# https://medium.com/datadriveninvestor/creating-a-pytorch-image-classifier-da9db139ba80\n",
    "transform = transforms.Compose([# size must be >= 224 \n",
    "                                transforms.Resize(256), \n",
    "                                transforms.CenterCrop(224), \n",
    "                                transforms.ToTensor(), # Convert the image to a tensor with pixels in the range [0, 1]\n",
    "                                # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with Imagenet parameters\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = SnakeDataset(X_val, y_val, root_dir = train_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SnakeDataset at 0x7f5cc96a8d30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    batch_size=len(X_val), #hparams['val_batch_size'], \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracies, eval_losses = [], []\n",
    "eval_loss = AverageMeter()\n",
    "eval_accuracy = AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): Flatten()\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=85, bias=True)\n",
       "    (4): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "pretrained_model = vgg16(pretrained=True)\n",
    "\n",
    "feature_extractor = pretrained_model.features\n",
    "feature_classifier = nn.Sequential(\n",
    "        nn.Linear(7*7*512, 256), # nn.Linear(8 * 8 * 64, 512), # 4*4*\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256,len(dict_labels)), # Number of different labels to classify our images into\n",
    "        nn.LogSoftmax(dim=-1) # From Activitat 6\n",
    ")\n",
    "\n",
    "model = nn.Sequential(\n",
    "        feature_extractor,\n",
    "        nn.Flatten(),\n",
    "        feature_classifier\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): Flatten()\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=85, bias=True)\n",
       "    (4): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "#load model\n",
    "model.load_state_dict(torch.load(\"second_model_v2.pt\"))\n",
    "# model = TempModel() # model = model()\n",
    "#model.load_state_dict(torch.load('second_model.pt', map_location=map_location))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(feature_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# As we are dealing here with a multiple class problem, we need to use torch.nn.CrossEntropyLoss() as the loss\n",
    "# function (see https://stackoverflow.com/questions/56821729/pytorch-bceloss-valueerror-target-and-input-must-have-the-same-number-of-ele).\n",
    "# loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some hyper-parameters\n",
    "hparams = {\n",
    "    'batch_size': 100, #  40, \n",
    "    'num_epochs':12,\n",
    "    'val_batch_size': 100, #  40, \n",
    "    'learning_rate':1e-3,\n",
    "    'log_interval':100,\n",
    "}\n",
    "\n",
    "# we select to work on GPU if it is available in the machine, otherwise\n",
    "# will run on CPU\n",
    "hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "epoch = hparams['num_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aac02f292e45fbb00e6c70f11d2229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.11 GiB (GPU 0; 11.17 GiB total capacity; 3.34 GiB already allocated; 2.18 GiB free; 8.69 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-15ef72a5d4b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#target = target.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.11 GiB (GPU 0; 11.17 GiB total capacity; 3.34 GiB already allocated; 2.18 GiB free; 8.69 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#model.eval()\n",
    "eval_loss.reset()\n",
    "eval_accuracy.reset()\n",
    "val_loop = tqdm(eval_loader, unit=\" batches\")  # For printing the progress bar\n",
    "with torch.no_grad():\n",
    "    # for data, target in val_loop:\n",
    "    for item in val_loop:\n",
    "        #val_loop.set_description('[VAL] Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        #data, target = data.float().to(device), target.float().to(device)\n",
    "        data, target = item['image'].float().to(device), item['labels'].float().to(device)\n",
    "        target = target.unsqueeze(-1)\n",
    "        #target = target.squeeze(1)\n",
    "        output = model(data)\n",
    "        target = target.squeeze(1)\n",
    "\n",
    "        loss = loss_fn(output.double(), target.long())\n",
    "        eval_loss.update(loss.item(), n=len(target))\n",
    "\n",
    "        pred = output.round()  # get the prediction\n",
    "        class_pred=torch.argmax(pred, dim=1)\n",
    "        acc = (class_pred==target).sum().item()/len(target)\n",
    "        eval_accuracy.update(acc, n=len(target))\n",
    "        val_loop.set_postfix(loss=eval_loss.avg, accuracy=eval_accuracy.avg)\n",
    "\n",
    "eval_losses.append(eval_loss.avg)\n",
    "eval_accuracies.append(eval_accuracy.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def challenge_extract_features(dataset, loader, batch_size): # dataset = trainset, testset, valset; loader = train_loader, test_loader, val_loader\n",
    "                                                   # batch_size = number_of_images_to_predict!!!\n",
    "    features = np.zeros(shape=(len(dataset), 512, 7, 7)) # 4, 4\n",
    "    # labels = np.zeros(shape=(len(dataset),))\n",
    "    hashedids = [] #np.zeros(shape=(len(dataset),))\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(loader):\n",
    "            # print(i)\n",
    "            inputs_batch = sample_batched['image']\n",
    "            #labels_batch = sample_batched['labels']\n",
    "            hashedids_batch = sample_batched['hashed_id']\n",
    "            # inputs_batch, labels_batch = inputs_batch.to(device), labels_batch.to(device)\n",
    "            inputs_batch = inputs_batch.to(device)\n",
    "            # print(\"Entering feature_extractor function\")\n",
    "            features_batch = feature_extractor(inputs_batch)\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            # labels[i * batch_size : (i + 1) * batch_size] = labels_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            # hashedids[i * batch_size : (i + 1) * batch_size] = hashedids_batch#.cpu().detach().numpy() #hashedids_batch.cpu().detach().numpy()\n",
    "            hashedids.extend(hashedids_batch)\n",
    "            \n",
    "    return features, hashedids #, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix - PENDING!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(challenge_class_pred.cpu().numpy(), columns=['label_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = '/Users/marta/Dropbox/DataScience/DeepLearning_PQTM/UnsupervisedLearning/Activity_I/'\n",
    "\n",
    "plot_confusion_matrix(cm           = confusion_matrix(df_pred['label_pred'], df_pred['label_pred']), \n",
    "                      normalize    = False,\n",
    "                      target_names = ['acc', 'unacc'], # true_negative, true_positive\n",
    "                      save_to_path = dataset_dir, # local_path,\n",
    "                      fig_name = 'ConfusionMatrix_first_model.png',\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST A COPY FROM PREVIOUS CELLS, YOU NEED TO BE CAREFUL WITH \n",
    "def extract_features(dataset, loader, batch_size): # dataset = trainset, testset, valset; loader = train_loader, test_loader, val_loader\n",
    "                                                   # batch_size = hparams['batch_size']\n",
    "    features = np.zeros(shape=(len(dataset), 512, 7, 7)) # 4, 4\n",
    "    labels = np.zeros(shape=(len(dataset),))\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(loader):\n",
    "            # print(i)\n",
    "            inputs_batch = sample_batched['image']\n",
    "            labels_batch = sample_batched['labels']\n",
    "            inputs_batch, labels_batch = inputs_batch.to(device), labels_batch.to(device)\n",
    "            # print(\"Entering feature_extractor function\")\n",
    "            features_batch = feature_extractor(inputs_batch)\n",
    "            # print(i * batch_size)\n",
    "            # print((i + 1) * batch_size)\n",
    "            # print(features_batch.cpu().detach().numpy().shape)\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch.cpu().detach().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            #features[i * batch_size : (i + 1) * batch_size] = features_batch.cpu().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "            #labels[i * batch_size : (i + 1) * batch_size] = labels_batch.cpu().numpy() # CAREFUL WITH .detach() HERE!!! NO GRADIENT WILL BE COMPUTED (so no backprop)!!!.numpy()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dlai-2019-lab09-transfer_todo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
